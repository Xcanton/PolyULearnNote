# Chapter7 Neural Network

## Logistic Regression

$$
y=h_\theta(x)=\sigma(\theta^\tau x) \\ where\ \sigma(\alpha)=\frac 1 {1+\exp(-\alpha)}
$$

<figure><img src="../../.gitbook/assets/image (158).png" alt=""><figcaption><p>Sigmoid Function</p></figcaption></figure>

## Tanh

<figure><img src="../../.gitbook/assets/Image_20231220170450.png" alt=""><figcaption><p>Tanh Function</p></figcaption></figure>

## ReLU & Weak ReLU

<figure><img src="../../.gitbook/assets/Image_20231220170556.png" alt=""><figcaption><p>ReLU and Weak ReLU</p></figcaption></figure>

## Soft Sigmoid

<figure><img src="../../.gitbook/assets/Image_20231220170653.png" alt=""><figcaption><p>Soft Sigmoid</p></figcaption></figure>

## Activation Functions

<figure><img src="../../.gitbook/assets/Image_20231220172603.png" alt=""><figcaption></figcaption></figure>

## Softmax

$$
y_k = \frac {\exp(b_k)} {\sum^K_{l=1}\exp(b_l)}
$$
