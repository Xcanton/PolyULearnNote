# N-Gram Language Models

## [#li-ti-ji-suan-ju-zi-gai-lv](n-grams/#li-ti-ji-suan-ju-zi-gai-lv "mention")

## [#li-ti-ji-suan-la-pu-la-si-ping-hua](smoothing-techniques.md#li-ti-ji-suan-la-pu-la-si-ping-hua "mention")

## <mark style="color:red;">作业</mark>：计算一段文本的N-Gram概率

<figure><img src="../../.gitbook/assets/image (2) (1).png" alt=""><figcaption></figcaption></figure>

<figure><img src="../../.gitbook/assets/image (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## <mark style="color:red;">作业</mark>：根据上述概率计算词条件概率

{% hint style="info" %}
Referring to the table provided in Exercise 1, what is 𝑃(food | i want Chinese) when using the Bi-gram model?
{% endhint %}

<figure><img src="../../.gitbook/assets/image (2) (1) (1).png" alt=""><figcaption></figcaption></figure>

## <mark style="color:red;">作业</mark>：根据场景计算困惑度

{% hint style="info" %}
Considering a training set of 10 numbers that consists digits 0 to 9, let us examine the following test set: 66. What is the perplexity of this test set using a Uni-gram model?
{% endhint %}

<figure><img src="../../.gitbook/assets/image (4) (1).png" alt=""><figcaption></figcaption></figure>
