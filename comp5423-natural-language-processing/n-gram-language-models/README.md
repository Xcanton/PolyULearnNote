# N-Gram Language Models

## [#li-ti-ji-suan-ju-zi-gai-lv](n-grams/#li-ti-ji-suan-ju-zi-gai-lv "mention")

## [#li-ti-ji-suan-la-pu-la-si-ping-hua](smoothing-techniques.md#li-ti-ji-suan-la-pu-la-si-ping-hua "mention")

## <mark style="color:red;">ä½œä¸š</mark>ï¼šè®¡ç®—ä¸€æ®µæ–‡æœ¬çš„N-Gramæ¦‚ç‡

<figure><img src="../../.gitbook/assets/image (2) (1).png" alt=""><figcaption></figcaption></figure>

<figure><img src="../../.gitbook/assets/image (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## <mark style="color:red;">ä½œä¸š</mark>ï¼šæ ¹æ®ä¸Šè¿°æ¦‚ç‡è®¡ç®—è¯æ¡ä»¶æ¦‚ç‡

{% hint style="info" %}
Referring to the table provided in Exercise 1, what is ğ‘ƒ(food | i want Chinese) when using the Bi-gram model?
{% endhint %}

<figure><img src="../../.gitbook/assets/image (2) (1) (1).png" alt=""><figcaption></figcaption></figure>

## <mark style="color:red;">ä½œä¸š</mark>ï¼šæ ¹æ®åœºæ™¯è®¡ç®—å›°æƒ‘åº¦

{% hint style="info" %}
Considering a training set of 10 numbers that consists digits 0 to 9, let us examine the following test set: 66. What is the perplexity of this test set using a Uni-gram model?
{% endhint %}

<figure><img src="../../.gitbook/assets/image (4) (1).png" alt=""><figcaption></figcaption></figure>
